{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/05/10 12:40:34 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"PYSPARK_PYTHON\"] = \"/Users/ashwinsahay/rearc_data_quest/.venv/bin/python\"\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = \"/Users/ashwinsahay/rearc_data_quest/.venv/bin/python\"\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"PopulationAnalytics\").getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA READING START"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- series_id: string (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- period: string (nullable = true)\n",
      " |-- value: double (nullable = true)\n",
      " |-- footnote_codes: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#READ ts_data\n",
    "ts_data = spark.read.csv('/Users/ashwinsahay/rearc_data_quest/data/pr.data.0.Current', \n",
    "                    sep=\"\\t\",\n",
    "                    header=True,\n",
    "                    inferSchema=True)\n",
    "\n",
    "# Strip leading/trailing whitespace from all column names\n",
    "ts_data = ts_data.toDF(*[col_name.strip() for col_name in ts_data.columns])\n",
    "\n",
    "# ts_data=ts_data.withColumn('series_id',trim(col(\"series_id\")))\n",
    "# ts_data.show(5)\n",
    "\n",
    "ts_data.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"/Users/ashwinsahay/rearc_data_quest/data/population_data.json\") as f:\n",
    "    json_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "records = json_data[\"data\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'ID Nation': '01000US', 'Nation': 'United States', 'ID Year': 2023, 'Year': '2023', 'Population': 332387540, 'Slug Nation': 'united-states'}, {'ID Nation': '01000US', 'Nation': 'United States', 'ID Year': 2022, 'Year': '2022', 'Population': 331097593, 'Slug Nation': 'united-states'}, {'ID Nation': '01000US', 'Nation': 'United States', 'ID Year': 2021, 'Year': '2021', 'Population': 329725481, 'Slug Nation': 'united-states'}, {'ID Nation': '01000US', 'Nation': 'United States', 'ID Year': 2020, 'Year': '2020', 'Population': 326569308, 'Slug Nation': 'united-states'}, {'ID Nation': '01000US', 'Nation': 'United States', 'ID Year': 2019, 'Year': '2019', 'Population': 324697795, 'Slug Nation': 'united-states'}, {'ID Nation': '01000US', 'Nation': 'United States', 'ID Year': 2018, 'Year': '2018', 'Population': 322903030, 'Slug Nation': 'united-states'}, {'ID Nation': '01000US', 'Nation': 'United States', 'ID Year': 2017, 'Year': '2017', 'Population': 321004407, 'Slug Nation': 'united-states'}, {'ID Nation': '01000US', 'Nation': 'United States', 'ID Year': 2016, 'Year': '2016', 'Population': 318558162, 'Slug Nation': 'united-states'}, {'ID Nation': '01000US', 'Nation': 'United States', 'ID Year': 2015, 'Year': '2015', 'Population': 316515021, 'Slug Nation': 'united-states'}, {'ID Nation': '01000US', 'Nation': 'United States', 'ID Year': 2014, 'Year': '2014', 'Population': 314107084, 'Slug Nation': 'united-states'}, {'ID Nation': '01000US', 'Nation': 'United States', 'ID Year': 2013, 'Year': '2013', 'Population': 311536594, 'Slug Nation': 'united-states'}]\n"
     ]
    }
   ],
   "source": [
    "print(records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+-------------+----------+-------------+----+\n",
      "|ID Nation|ID Year|       Nation|Population|  Slug Nation|Year|\n",
      "+---------+-------+-------------+----------+-------------+----+\n",
      "|  01000US|   2023|United States| 332387540|united-states|2023|\n",
      "|  01000US|   2022|United States| 331097593|united-states|2022|\n",
      "|  01000US|   2021|United States| 329725481|united-states|2021|\n",
      "|  01000US|   2020|United States| 326569308|united-states|2020|\n",
      "|  01000US|   2019|United States| 324697795|united-states|2019|\n",
      "+---------+-------+-------------+----------+-------------+----+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- ID Nation: string (nullable = true)\n",
      " |-- ID Year: long (nullable = true)\n",
      " |-- Nation: string (nullable = true)\n",
      " |-- Population: long (nullable = true)\n",
      " |-- Slug Nation: string (nullable = true)\n",
      " |-- Year: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "population_df = spark.createDataFrame(records)\n",
    "population_df.show(5)\n",
    "\n",
    "\n",
    "population_data = population_df.withColumn(\"Year\", col(\"Year\").cast(\"int\")) \n",
    "population_data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DATA READ COMPLETE "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DATA ANALYSIS BELOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----------------+\n",
      "|Mean_Population|StdDev_Population|\n",
      "+---------------+-----------------+\n",
      "|   3.17437383E8|     4257089.5415|\n",
      "+---------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Task 1: Mean and Std Dev of US Population (2013â€“2018)\n",
    "from pyspark.sql.functions import col,avg,stddev,round,sum\n",
    "population_filtered = population_data.filter((col(\"Year\") >= 2013) & (col(\"Year\") <= 2018))\n",
    "\n",
    "population_filtered.select(\n",
    "    round(avg(\"Population\"),4).alias(\"Mean_Population\"),\n",
    "    round(stddev(\"Population\"),4).alias(\"StdDev_Population\")\n",
    ").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----+------------------+\n",
      "|series_id  |year|value             |\n",
      "+-----------+----+------------------+\n",
      "|PRS30006011|2022|20.5              |\n",
      "|PRS30006012|2022|17.1              |\n",
      "|PRS30006013|1998|705.895           |\n",
      "|PRS30006021|2010|17.7              |\n",
      "|PRS30006022|2010|12.399999999999999|\n",
      "|PRS30006023|2014|503.21600000000007|\n",
      "|PRS30006031|2022|20.4              |\n",
      "|PRS30006032|2021|17.1              |\n",
      "|PRS30006033|1998|702.672           |\n",
      "|PRS30006061|2022|37.0              |\n",
      "|PRS30006062|2021|31.6              |\n",
      "|PRS30006063|2024|647.479           |\n",
      "|PRS30006081|2021|24.4              |\n",
      "|PRS30006082|2021|24.4              |\n",
      "|PRS30006083|2021|110.742           |\n",
      "|PRS30006091|2002|43.3              |\n",
      "|PRS30006092|2002|44.39999999999999 |\n",
      "|PRS30006093|2013|514.1560000000001 |\n",
      "|PRS30006101|2020|33.5              |\n",
      "|PRS30006102|2020|36.2              |\n",
      "+-----------+----+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Task 2: Best Year by Total Value per Series ID\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import sum as sum, row_number, trim\n",
    "\n",
    "# Remove whitespace from series_id\n",
    "ts_data = ts_data.withColumn(\"series_id\", trim(col(\"series_id\")))\n",
    "\n",
    "# Sum value by series_id and year\n",
    "aggregated = ts_data.groupBy(\"series_id\", \"year\").agg(sum(\"value\").alias(\"year_total\"))\n",
    "\n",
    "# Use row_number to pick year with max total for each series\n",
    "window_spec = Window.partitionBy(\"series_id\").orderBy(col(\"year_total\").desc())\n",
    "\n",
    "best_years = aggregated.withColumn(\"rank\", row_number().over(window_spec)) \\\n",
    "                       .filter(col(\"rank\") == 1) \\\n",
    "                       .select(\"series_id\", \"year\", col(\"year_total\").alias(\"value\"))\n",
    "\n",
    "best_years.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----+------+-----+----------+\n",
      "|series_id  |year|period|value|Population|\n",
      "+-----------+----+------+-----+----------+\n",
      "|PRS30006032|2003|Q01   |-5.7 |NULL      |\n",
      "|PRS30006032|2007|Q01   |-0.8 |NULL      |\n",
      "|PRS30006032|2015|Q01   |-1.7 |316515021 |\n",
      "|PRS30006032|2006|Q01   |1.8  |NULL      |\n",
      "|PRS30006032|2013|Q01   |0.5  |311536594 |\n",
      "|PRS30006032|1997|Q01   |2.8  |NULL      |\n",
      "|PRS30006032|2014|Q01   |-0.1 |314107084 |\n",
      "|PRS30006032|2004|Q01   |2.0  |NULL      |\n",
      "|PRS30006032|1996|Q01   |-4.2 |NULL      |\n",
      "|PRS30006032|1998|Q01   |0.9  |NULL      |\n",
      "|PRS30006032|2012|Q01   |2.5  |NULL      |\n",
      "|PRS30006032|2009|Q01   |-21.0|NULL      |\n",
      "|PRS30006032|1995|Q01   |0.0  |NULL      |\n",
      "|PRS30006032|2001|Q01   |-6.3 |NULL      |\n",
      "|PRS30006032|2005|Q01   |-0.5 |NULL      |\n",
      "|PRS30006032|2000|Q01   |0.5  |NULL      |\n",
      "|PRS30006032|2010|Q01   |3.2  |NULL      |\n",
      "|PRS30006032|2011|Q01   |1.5  |NULL      |\n",
      "|PRS30006032|2008|Q01   |-3.5 |NULL      |\n",
      "|PRS30006032|1999|Q01   |-4.1 |NULL      |\n",
      "+-----------+----+------+-----+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Task 3: Join PRS30006032 Q01 Records with Population\n",
    "\n",
    "# Filter time-series data\n",
    "target_series = ts_data.filter(\n",
    "    (trim(col(\"series_id\")) == \"PRS30006032\") &\n",
    "    (trim(col(\"period\")) == \"Q01\")\n",
    ")\n",
    "\n",
    "# Prepare population for join\n",
    "population_df = population_data.withColumnRenamed(\"Year\", \"year\")\n",
    "\n",
    "# Join\n",
    "joined = target_series.join(population_df, on=\"year\", how=\"left\") \\\n",
    "                      .select(\"series_id\", \"year\", \"period\", \"value\", \"Population\")\n",
    "\n",
    "joined.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA ANALYSIS COMPLETE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pyspark)",
   "language": "python",
   "name": "pyspark-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
